{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "apart-burlington",
   "metadata": {},
   "source": [
    "### Load Stanford movie review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "processed-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "improving-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "impressed-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  getfilelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory\"\"\"\n",
    "    path = Path(root)\n",
    "    textfiles = path.glob('**/*.txt')\n",
    "    return [str(line) for line in textfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distinct-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettext(filename):\n",
    "    \"\"\"Return a string text from given txt file\"\"\"\n",
    "    with open(filename) as f:\n",
    "        text = f.read().replace(\"<br />\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "statutory-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettexttodf(rootpath):\n",
    "    \"\"\"Extract text from all the txt files under given directory\n",
    "       and return a dataframe with string reviews and labels\"\"\"\n",
    "    filename_list = getfilelist(rootpath)\n",
    "    # identify data from positive or negative dataset\n",
    "    if \"neg\" in rootpath:\n",
    "        label = np.zeros(len(filename_list), dtype=int)\n",
    "    else:\n",
    "        label = np.ones(len(filename_list), dtype=int)\n",
    "    \n",
    "    review = []\n",
    "    for filename in filename_list:\n",
    "        review.append(gettext(filename))\n",
    "    \n",
    "    return (review, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "pregnant-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_path = \"aclImdb/small_data/neg/\"\n",
    "pos_path = \"aclImdb/small_data/pos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "agricultural-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_content, neg_y = gettexttodf(neg_path)\n",
    "pos_content, pos_y = gettexttodf(pos_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "separated-paradise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-plastic",
   "metadata": {},
   "source": [
    " ### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "better-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w for w in tokens if (len(w) > 3) and (w not in ENGLISH_STOP_WORDS)]  # ignore a, an, to, at, be, ...\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coupled-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizewords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list of normalize words\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    normal = []\n",
    "    for word, tag in nltk.pos_tag(words):\n",
    "        wtag = tag[0].lower()\n",
    "        wtag = wtag if wtag in ['a', 'r', 'n', 'v'] else None\n",
    "        lemma = lemmatizer.lemmatize(word, wtag) if wtag else word\n",
    "        normal.append(lemma)\n",
    "    return ' '.join(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accomplished-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Technically abominable (with audible \"pops\" between scenes)and awesomely amateurish, \"Flesh\" requires a lot of patience to sit through and will probably turn off most viewers; but the dialogue rings amazingly true and Joe Dallesandro, who exposes his body in almost every scene, also gives an utterly convincing performance. A curio, to be sure, but the more polished \"Trash\", made two years later, is a definite step forward. I suggest you watch that instead. (*1/2) \n",
      " After:  technically abominable audible pop scene awesomely amateurish flesh require patience probably turn viewer dialogue ring amazingly true dallesandro expose body scene give utterly convincing performance curio sure polish trash year later definite step forward suggest watch instead\n"
     ]
    }
   ],
   "source": [
    "# example output\n",
    "print('Before: ', neg_content[0],'\\n','After: ', normalizewords(tokenize(neg_content[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "electrical-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "neg_content = [normalizewords(tokenize(line)) for line in neg_content]\n",
    "pos_content = [normalizewords(tokenize(line)) for line in pos_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-phrase",
   "metadata": {},
   "source": [
    "### Split dataset in train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "broken-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(neg_content, pos_content)\n",
    "y = np.append(neg_y, pos_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "crude-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-dependence",
   "metadata": {},
   "source": [
    "### Words to Index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "brave-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "spread-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the vocabulary from the training set\n",
    "word_count = get_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "direct-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "endangered-westminster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10024"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "incomplete-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(word_count):\n",
    "    if word_count[word] < 4:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "monetary-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2609"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "settled-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we need an index for each word in the vocab\n",
    "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
    "words = [\"<PAD>\", \"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "controversial-kennedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "wanted-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-chapter",
   "metadata": {},
   "source": [
    "### Sentance Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "sound-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each number is the length of the sentence\n",
    "x_train_len = np.array([len(x.split()) for x in X_train]) \n",
    "x_val_len = np.array([len(x.split()) for x in X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "hungry-compensation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(x_train_len, 90) # set max len to 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "reflected-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "subject-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=max_len):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    # use index to represent words\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    # cut it length > N\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "defensive-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_len = np.minimum(x_train_len, max_len)\n",
    "x_val_len = np.minimum(x_val_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "fundamental-operator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 200)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "finite-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 200)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = np.vstack([encode_sentence(x) for x in X_val])\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-parent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vocational-amount",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "republican-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        # initialize random embedding matrix\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_size, 1)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.sum(dim=1)/ s\n",
    "        x = self.linear(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "pacific-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (word_emb): Embedding(5, 4, padding_idx=0)\n",
       "  (linear): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW(vocab_size=5, emb_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "premier-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2611\n"
     ]
    }
   ],
   "source": [
    "V = len(words)\n",
    "model = CBOW(vocab_size=V, emb_size=50)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "ordered-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    x = torch.LongTensor(x_val) #.cuda()\n",
    "    y = torch.Tensor(y_val).unsqueeze(1) #).cuda()\n",
    "    s = torch.Tensor(x_val_len).view(x_val_len.shape[0], 1)\n",
    "    y_hat = model(x, s)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    return loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "spiritual-equation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6979413032531738, 0.5)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model) # about 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "balanced-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        x = torch.LongTensor(x_train)  #.cuda()\n",
    "        y = torch.Tensor(y_train).unsqueeze(1)\n",
    "        s = torch.Tensor(x_train_len).view(x_train_len.shape[0], 1)\n",
    "        y_hat = model(x, s) # predicted value\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y) # get loss\n",
    "        optimizer.zero_grad() # set the gradients to zero \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        val_loss, val_accuracy = val_metrics(model)\n",
    "        print(\"train_loss %.6f val_loss %.3f val_accuracy %.3f\" % (loss.item(), val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "renewable-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.690264 val_loss 0.660 val_accuracy 0.544\n",
      "train_loss 0.643663 val_loss 0.627 val_accuracy 0.663\n",
      "train_loss 0.547587 val_loss 0.570 val_accuracy 0.725\n",
      "train_loss 0.437712 val_loss 0.475 val_accuracy 0.831\n",
      "train_loss 0.317343 val_loss 0.410 val_accuracy 0.819\n",
      "train_loss 0.229734 val_loss 0.367 val_accuracy 0.850\n",
      "train_loss 0.148871 val_loss 0.369 val_accuracy 0.869\n",
      "train_loss 0.095424 val_loss 0.387 val_accuracy 0.856\n",
      "train_loss 0.062842 val_loss 0.375 val_accuracy 0.856\n",
      "train_loss 0.037675 val_loss 0.351 val_accuracy 0.856\n",
      "train_loss 0.022152 val_loss 0.338 val_accuracy 0.869\n",
      "train_loss 0.014140 val_loss 0.338 val_accuracy 0.875\n",
      "train_loss 0.009159 val_loss 0.348 val_accuracy 0.875\n",
      "train_loss 0.005351 val_loss 0.364 val_accuracy 0.881\n",
      "train_loss 0.002775 val_loss 0.385 val_accuracy 0.875\n",
      "train_loss 0.001479 val_loss 0.409 val_accuracy 0.869\n",
      "train_loss 0.000880 val_loss 0.434 val_accuracy 0.869\n",
      "train_loss 0.000577 val_loss 0.459 val_accuracy 0.863\n",
      "train_loss 0.000407 val_loss 0.484 val_accuracy 0.863\n",
      "train_loss 0.000304 val_loss 0.508 val_accuracy 0.863\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "classical-representation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5076395273208618, 0.862500011920929)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "forbidden-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size=V, emb_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "solved-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.690807 val_loss 0.685 val_accuracy 0.575\n",
      "train_loss 0.673843 val_loss 0.672 val_accuracy 0.625\n",
      "train_loss 0.655129 val_loss 0.663 val_accuracy 0.663\n",
      "train_loss 0.630417 val_loss 0.651 val_accuracy 0.675\n",
      "train_loss 0.600020 val_loss 0.632 val_accuracy 0.706\n",
      "train_loss 0.563297 val_loss 0.612 val_accuracy 0.731\n",
      "train_loss 0.523042 val_loss 0.595 val_accuracy 0.738\n",
      "train_loss 0.479391 val_loss 0.579 val_accuracy 0.738\n",
      "train_loss 0.435078 val_loss 0.559 val_accuracy 0.744\n",
      "train_loss 0.390017 val_loss 0.538 val_accuracy 0.788\n",
      "train_loss 0.346103 val_loss 0.522 val_accuracy 0.775\n",
      "train_loss 0.303146 val_loss 0.510 val_accuracy 0.750\n",
      "train_loss 0.262440 val_loss 0.497 val_accuracy 0.762\n",
      "train_loss 0.224202 val_loss 0.481 val_accuracy 0.775\n",
      "train_loss 0.189282 val_loss 0.473 val_accuracy 0.781\n",
      "train_loss 0.157886 val_loss 0.471 val_accuracy 0.806\n",
      "train_loss 0.130416 val_loss 0.467 val_accuracy 0.806\n",
      "train_loss 0.106769 val_loss 0.460 val_accuracy 0.825\n",
      "train_loss 0.086825 val_loss 0.459 val_accuracy 0.812\n",
      "train_loss 0.070229 val_loss 0.465 val_accuracy 0.812\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "painted-porter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46491122245788574, 0.8125)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-dress",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "circular-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "primary-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence2(s, N=200):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc, l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fallen-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectivityDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        x, s = encode_sentence2(x)\n",
    "        return x, self.y[idx], s\n",
    "    \n",
    "sub_dataset_train = SubjectivityDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "representative-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=5, shuffle=True)\n",
    "x, y, s = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-furniture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "early-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate\n",
    "model = CBOW(vocab_size=V, emb_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "legitimate-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "arranged-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs_sgd(model, epochs=10, lr=0.02):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # what is optimizer?\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        for x, y, s in train_loader:\n",
    "            x = x.type(torch.LongTensor)  #.cuda()\n",
    "            y = y.type(torch.FloatTensor).unsqueeze(1)\n",
    "            s = s.type(torch.Tensor).view(s.shape[0], 1)\n",
    "            y_hat = model(x, s)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += x.size(0)*loss.item()\n",
    "            total += x.size(0)\n",
    "            train_loss = total_loss/total\n",
    "        val_loss, val_accuracy = val_metrics(model)\n",
    "        \n",
    "        print(\"train_loss %.3f val_loss %.3f val_accuracy %.3f\" % (train_loss, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "confidential-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.698 val_loss 0.676 val_accuracy 0.613\n",
      "train_loss 0.666 val_loss 0.674 val_accuracy 0.531\n",
      "train_loss 0.639 val_loss 0.656 val_accuracy 0.556\n",
      "train_loss 0.599 val_loss 0.609 val_accuracy 0.781\n",
      "train_loss 0.545 val_loss 0.567 val_accuracy 0.800\n",
      "train_loss 0.491 val_loss 0.527 val_accuracy 0.819\n",
      "train_loss 0.428 val_loss 0.486 val_accuracy 0.844\n",
      "train_loss 0.358 val_loss 0.457 val_accuracy 0.819\n",
      "train_loss 0.294 val_loss 0.436 val_accuracy 0.831\n",
      "train_loss 0.241 val_loss 0.407 val_accuracy 0.850\n",
      "train_loss 0.192 val_loss 0.368 val_accuracy 0.856\n",
      "train_loss 0.151 val_loss 0.339 val_accuracy 0.856\n",
      "train_loss 0.121 val_loss 0.326 val_accuracy 0.856\n",
      "train_loss 0.095 val_loss 0.324 val_accuracy 0.850\n",
      "train_loss 0.074 val_loss 0.331 val_accuracy 0.856\n",
      "train_loss 0.059 val_loss 0.332 val_accuracy 0.856\n",
      "train_loss 0.047 val_loss 0.324 val_accuracy 0.856\n",
      "train_loss 0.037 val_loss 0.314 val_accuracy 0.844\n",
      "train_loss 0.029 val_loss 0.308 val_accuracy 0.850\n",
      "train_loss 0.024 val_loss 0.309 val_accuracy 0.850\n"
     ]
    }
   ],
   "source": [
    "train_epocs_sgd(model, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-success",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "awful-poverty",
   "metadata": {},
   "source": [
    "## test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "grand-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_path = \"aclImdb/small_data/test_neg/\"\n",
    "test_pos_path = \"aclImdb/small_data/test_pos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "empirical-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_content, test_neg_y = gettexttodf(neg_path)\n",
    "test_pos_content, test_pos_y = gettexttodf(pos_path)\n",
    "content = test_neg_content+test_pos_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-ticket",
   "metadata": {},
   "source": [
    "### without pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "musical-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, content):\n",
    "    model.eval()\n",
    "    \n",
    "    code = []\n",
    "    x_len = []\n",
    "    for i in [encode_sentence2(x) for x in content]:\n",
    "        code.append(i[0])\n",
    "        x_len.append(i[1])\n",
    "        \n",
    "    x_len = np.minimum(x_len, 200)\n",
    "    x = torch.LongTensor(code) #.cuda()\n",
    "#     y = torch.Tensor(y_val).unsqueeze(1) #).cuda()\n",
    "    s = torch.Tensor(x_len).view(x_len.shape[0], 1)\n",
    "    y_hat = model(x, s)\n",
    "    \n",
    "    return (y_hat > 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "grave-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction(model, test_pos_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "criminal-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predict accuracy of positive is: 0.94\n"
     ]
    }
   ],
   "source": [
    "print(f'the predict accuracy of positive is: {(prediction(model, test_pos_content) == 1).sum()/len(pos_y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "sustained-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predict accuracy of negative is: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(f'the predict accuracy of negative is: {(prediction(model, test_neg_content) == 0).sum()/len(test_neg_y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-passenger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-candy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automotive-tomorrow",
   "metadata": {},
   "source": [
    "### with pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "mineral-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_content = [normalizewords(tokenize(line)) for line in test_pos_content]\n",
    "test_neg_content = [normalizewords(tokenize(line)) for line in test_neg_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "oriental-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, content):\n",
    "    model.eval()\n",
    "    \n",
    "    code = []\n",
    "    x_len = []\n",
    "    for i in [encode_sentence2(x) for x in content]:\n",
    "        code.append(i[0])\n",
    "        x_len.append(i[1])\n",
    "        \n",
    "    x_len = np.minimum(x_len, 200)\n",
    "    x = torch.LongTensor(code) #.cuda()\n",
    "#     y = torch.Tensor(y_val).unsqueeze(1) #).cuda()\n",
    "    s = torch.Tensor(x_len).view(x_len.shape[0], 1)\n",
    "    y_hat = model(x, s)\n",
    "    \n",
    "    return (y_hat > 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "small-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predict accuracy of positive is: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(f'the predict accuracy of positive is: {(prediction(model, test_pos_content) == 1).sum()/len(test_pos_y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "silent-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predict accuracy of negative is: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(f'the predict accuracy of negative is: {(prediction(model, test_neg_content) == 0).sum()/len(test_neg_y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-seller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
